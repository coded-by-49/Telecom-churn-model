{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIPSiasJnTHF9hhOK8zMTH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["  '''\n","  PUSING MY COLLAB PROJECT TO GITHUB\n","  '''\n","# !ssh-keygen -t rsa -b 4096 -C \"fabianbernard49@gmail.com\" -f ~/.ssh/id_rsa -N \"\"  #Create ssh pair key\n","# !cat ~/.ssh/id_rsa.pub #Print out your\n","!ssh-keyscan -H github.com >> ~/.ssh/known_hosts #Make github a knownhost\n","!ssh -T git@github.com #Aunthenticate github\n","\n","  # Verify your identity\n","!git config --global user.name \"coded-by-49\"\n","!git config --global user.email \"fabianbernard49@gmail.com\"\n","\n","# %cd /content/drive/MyDrive/Git_repos #Create a folder for git hub repos if its your first time\n","# %cd /content/drive/MyDrive/Git_repos/Telecom-churn-model/ #Enter that folder\n","!git clone git@github.com:coded-by-49/Telecom-churn-model.git  #Clone your repo into it\n","\n","!cp \"/content/drive/MyDrive/Colab Notebooks/Tele_churn.ipynb\" /content/drive/MyDrive/Git_repos/Telecom-churn-model/ #Copy your project note book into it\n","!git status\n","!git add Tele_churn.ipynb #Add the file itself to avoid json file upload in repo\n","!git commit -m \"Implementation and Testing of Random forest classifier\"\n","!git push origin main"],"metadata":{"id":"cTN7maFWg2rM","executionInfo":{"status":"ok","timestamp":1752676694846,"user_tz":-60,"elapsed":3070,"user":{"displayName":"Henry Aloh Fabian","userId":"00863304101285682802"}},"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"outputId":"bb12c3f4-52c0-4ca6-d18f-630d91762e8c"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["# github.com:22 SSH-2.0-934fdb2c\n","# github.com:22 SSH-2.0-934fdb2c\n","# github.com:22 SSH-2.0-934fdb2c\n","# github.com:22 SSH-2.0-934fdb2c\n","# github.com:22 SSH-2.0-934fdb2c\n","Hi coded-by-49! You've successfully authenticated, but GitHub does not provide shell access.\n","Cloning into 'Telecom-churn-model'...\n","remote: Enumerating objects: 22, done.\u001b[K\n","remote: Counting objects: 100% (22/22), done.\u001b[K\n","remote: Compressing objects: 100% (19/19), done.\u001b[K\n","remote: Total 22 (delta 5), reused 10 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (22/22), 11.83 KiB | 11.83 MiB/s, done.\n","Resolving deltas: 100% (5/5), done.\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n"]}]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330},"id":"bfQaQpg8efAJ","executionInfo":{"status":"ok","timestamp":1752676279289,"user_tz":-60,"elapsed":9851,"user":{"displayName":"Henry Aloh Fabian","userId":"00863304101285682802"}},"outputId":"261d29ad-aeb3-4ed2-c5c8-8e7b436d6e48","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Testing accuracy for sckit learns model: 0.7867803837953091\n","Testing accuracy for custom_log_regression : 0.7903340440653873\n","Confusion matrix of sk learn model = {'TP': 192, 'TN': 915, 'FP': 118, 'FN': 182}\n","Confusion matrix of custom_log_regression = {'TP': 171, 'TN': 941, 'FP': 92, 'FN': 203}\n","\n","\n","Accuracy on oversampled dataset prediction by custom_log_regression) = 0.7356076759061834\n","Confusion matrix based on oversampled dataset prediction by custom_log_regression= {'TP': 287, 'TN': 748, 'FP': 285, 'FN': 87}\n","\n","\n","Accuracy on oversampled dataset prediction by RandomForestClassifier = 0.7604832977967306\n","confusion matrix based on oversammpled dataset prediction by RandomForestClassifier  = {'TP': 242, 'TN': 828, 'FP': 205, 'FN': 132}\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nTesting result of oversampled dataset (ADASYIN SMOTE) set traning with RandomForestClassifier\\nRandomForestClassifier: 76% accuracy(balanced) with 64.7% (recall).\\n\\nWhich means RandomForestClassifier would not cut it for business standard\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}],"source":["from itertools import groupby\n","import numpy as np\n","import pandas as pd\n","from IPython.display import clear_output\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","'''Data Preprocessing'''\n","from google.colab import drive\n","import urllib\n","\n","# Importing file from googledrive\n","drive.mount('/content/drive')\n","tele_churn_csv = \"/content/drive/MyDrive/GOOGLE COLLAB PROJECTS/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n","df_tele = pd.read_csv(tele_churn_csv)\n","\n","# Cleaning out empty feature spots\n","df_tele[\"TotalCharges\"] = df_tele[\"TotalCharges\"].replace(\" \", np.nan)\n","df_tele.dropna(subset = [\"TotalCharges\"], inplace=True)\n","df_tele[\"TotalCharges\"] = pd.to_numeric(df_tele[\"TotalCharges\"])\n","\n","# Dropping unimportant features\n","df_tele.drop(\"customerID\", axis = 1, inplace = True) # removed this unimportant feature\n","\n","# Importation of sckitlearns logisticregression algorithm\n","from sklearn.linear_model import LogisticRegression\n","\n","# Encoding our dataframe\n","encoded_df_tele = pd.get_dummies(df_tele, drop_first= True )\n","encoded_df_tele.head()\n","\n","# Conversion of boolean columns into zeros and ones\n","bool_cols = encoded_df_tele.select_dtypes(include = \"bool\").columns\n","encoded_df_tele[bool_cols] = encoded_df_tele[bool_cols].astype(int)\n","\n","# Conversion of tenure datatype numerical datatype\n","encoded_df_tele[\"tenure\"] = encoded_df_tele[\"tenure\"].astype(float)\n","\n","# Storing all the numerical feature titles\n","all_numerical_features = encoded_df_tele.select_dtypes(include = np.number).columns.tolist()\n","needed_numerical_feature = [feature for feature in all_numerical_features if encoded_df_tele[feature].nunique()>2]\n","\n","# Separation of future and labels\n","labels = encoded_df_tele[\"Churn_Yes\"]\n","features = encoded_df_tele.drop(\"Churn_Yes\", axis = 1)\n","\n","# Creating our testing and training data\n","x_train, x_test, y_train, y_test = train_test_split(features,labels, test_size = 0.2, random_state = 42 )\n","\n","# Scaling our non-categoical numerical features\n","Scaler = StandardScaler()\n","x_train[needed_numerical_feature] = Scaler.fit_transform(x_train[needed_numerical_feature])\n","x_test[needed_numerical_feature] = Scaler.transform(x_test[needed_numerical_feature])\n","\n","'''Training our models'''\n","log_regression = LogisticRegression(max_iter=1000)\n","log_regression.fit(x_train, y_train)\n","\n","# Testing the model for accuracy\n","sk_predicted_class = log_regression.predict(x_test)\n","\n","\n","\"\"\"IMPLEMENTATION OF LOGISTIC REGRESSION ALGORITHM FROM SCRATCH\"\"\"\n","class custom_Log_regression:\n","  '''\n","      Attributes:\n","        learning_rate (float): Step size for gradient descent updates.\n","        n_iters (int): Maximum number of iterations for training.\n","  '''\n","  def __init__(self, learning_rate = 0.01, n_iters = 1000):\n","    self.learning_rate = learning_rate\n","    self.n_iters = n_iters\n","\n","  def sigmoid(self,z):\n","    '''\n","       Sigmoid function, used for transformation of numerical prediction to 1's and 0's\n","       Args:\n","            z (numpy.ndarray): Input values (linear combination of features and weights).\n","       Our sigmoid then transfroms it into an array numpy.ndarray of probablilities between 0 and 1\n","    '''\n","    return 1/(1+np.exp(-z))\n","\n","  def fit(self,X,Y):\n","    '''\n","    (Training function) Fit function would derive the weight and bias that\n","     occurs during the two minimum derivative of the loss function\n","\n","    Args:\n","        X (numpy.ndarray): Training features (shape: [n_samples, n_features]).\n","        Y (numpy.ndarray): Training labels (shape: [n_samples]).\n","    '''\n","    self.num_samples, self.num_features = X.shape\n","    self.weights = np.zeros(self.num_features)\n","    self.bias = 0\n","\n","    for _ in range(self.n_iters):\n","      y_num_prediction = np.dot(X, self.weights) + self.bias\n","      y_predicted = self.sigmoid(y_num_prediction)\n","\n","      # Derivative of loss function\n","      dw = np.dot(X.T,(y_predicted - Y))/self.num_samples\n","      db = np.mean((y_predicted - Y))\n","\n","      # Adjustment weights and bias after each iteration\n","      self.weights = self.weights - self.learning_rate * dw\n","      self.bias = self.bias - self.learning_rate * db\n","\n","      # Stopping condition with eculidean normalisation\n","      if np.linalg.norm(db) < 0.001 and np.linalg.norm(dw) < 0.001:\n","        break # Minimum has been derived\n","\n","  def score(self, x_test,y_test):\n","    '''\n","    Scoring function which tells us the confusion metrics (Accuracy) of our model\n","    '''\n","\n","    # Deducing model prediction\n","    y_num_prediction = np.dot(x_test, self.weights) + self.bias\n","    self.y_predicted = self.sigmoid(y_num_prediction)\n","\n","    # Addition of decision thresold to get standard decsion (1 or 0)\n","    self.y_predicted = np.where(self.y_predicted > 0.5, 1, 0)\n","    accuracy =  np.mean(self.y_predicted == y_test)\n","    return accuracy\n","\n","  def confusion_matrix(self,predicted_class,actual_class):\n","    '''\n","      The Accuracy/scoring of our model is not enough for solving the business problem\n","\n","      Confusion metrics would there procuce more insights like:\n","      Sensitivity and Precision\n","    '''\n","    confusion_matrix = {\n","    'TP': 0,\n","    'TN': 0,\n","    'FP': 0,\n","    'FN': 0\n","    }\n","    for i in range(len(actual_class)):\n","      if predicted_class[i] == actual_class.iloc[i]:\n","        if predicted_class[i] == 1:\n","            confusion_matrix[\"TP\"] += 1\n","        else:\n","            confusion_matrix[\"TN\"] += 1\n","      else:\n","        if actual_class.iloc[i] == 0:\n","            confusion_matrix[\"FP\"] += 1\n","        else:\n","            confusion_matrix[\"FN\"] += 1\n","    return confusion_matrix\n","\n","\n","'''Testing our model against the sckit learn version'''\n","custom_model = custom_Log_regression()\n","custom_model.fit(x_train, y_train)\n","\n","print(f\"Testing accuracy for sckit learns model: {log_regression.score(x_test, y_test)}\")\n","print(f\"Testing accuracy for custom_log_regression : {custom_model.score(x_test,y_test)}\")\n","\n","'''\n","Scikit-learn Logistic Regression: 79% accuracy.\n","Custom Logistic Regression: 79/80% accuracY.\n","'''\n","\n","'''Testing the confusion matrix'''\n","# Accessing the predicted value of y from within my custom model\n","predicted_class = custom_model.y_predicted\n","\n","# Accessing the actual value of y from within my custom model\n","actual_class = y_test\n","\n","print(f\"Confusion matrix of sk learn model = {custom_model.confusion_matrix(sk_predicted_class,actual_class)}\")\n","print(f\"Confusion matrix of custom_log_regression = {custom_model.confusion_matrix(predicted_class,actual_class)}\\n\\n\")\n","\n","''' So from our confusion matrix test, we can tell that our data is skwed, because of the poor recall/senstivity (a whooping 45%). '''\n","\n","''' Implementation of Adasyin-SMOTE\n","    We want to find most skwed points of class and generate sample features and labels there\n","    However this algorithm is going to be fitted only to our training data as the main aim is to builds to models prediction.\n","'''\n","\n","# Importing algorithm\n","from imblearn.over_sampling import ADASYN\n","adasyn = ADASYN(random_state=42)\n","\n","# Resampling of Futures and Lables\n","x_train_bal, y_train_bal = adasyn.fit_resample(x_train,y_train)\n","\n","# Fitting our model with the new Dataset\n","custom_model.fit(x_train_bal,y_train_bal)\n","print(f\"Accuracy on oversampled dataset prediction by custom_log_regression) = {custom_model.score(x_test,y_test)}\")\n","\n","predicted_class = custom_model.y_predicted\n","actual_class = y_test\n","\n","print(f\"Confusion matrix based on oversampled dataset prediction by custom_log_regression= {custom_model.confusion_matrix(predicted_class,actual_class)}\\n\\n\")\n","\n","'''\n","Testing result of oversampled dataset (ADASYIN SMOTE) traning with custom_log_regression\n","Custom Logistic Regression: 73% accuracy(balanced) with 76.7% (recall).\n","\n","Which means logistic regression would not cut it for business standard\n","'''\n","\n","\"\"\"IMPLEMENATION OF RANDOM FOREST CLASSIFIER\"\"\"\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix\n","# train model\n","sk_rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","sk_rf_model.fit(x_train_bal,y_train_bal)\n","\n","#access predicted value of y\n","y_pred = sk_rf_model.predict(x_test)\n","\n","# test our model\n","print(f\"Accuracy on oversampled dataset prediction by RandomForestClassifier = {sk_rf_model.score(x_test,y_test)}\")\n","\n","print(f\"confusion matrix based on oversammpled dataset prediction by RandomForestClassifier  = {custom_model.confusion_matrix(y_pred,y_test)}\")\n","\n","'''\n","Testing result of oversampled dataset (ADASYIN SMOTE) training with RandomForestClassifier\n","RandomForestClassifier: 76% accuracy(balanced) with 64.7% (recall).\n","\n","Which means RandomForestClassifier would not cut it for business standard\n","'''\n"]},{"cell_type":"code","source":[],"metadata":{"id":"F-D4IEQUe-V6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Iiem80w5AoSx"},"execution_count":null,"outputs":[]}]}