{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1oGemtDSZlkkLbD3DnJGBpGikPkftxJ7I",
      "authorship_tag": "ABX9TyORgTWov6/EOcWVDfUgj+r8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coded-by-49/Telecom-churn-model/blob/main/Tele_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vMaDxPOapKgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import groupby\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from IPython.display import clear_output\n",
        "import tensorflow as tf\n",
        "fc = tf.compat.v2.feature_column\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# GET OUR THE FILE READY\n",
        "from google.colab import drive\n",
        "import urllib\n",
        "drive.mount('/content/drive')\n",
        "tele_file = \"/content/drive/MyDrive/GOOGLE COLLAB PROJECTS/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "df_tele = pd.read_csv(tele_file)\n",
        "\n",
        "# LEARNIN ALGORITHM\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "df_tele.drop(\"customerID\", axis = 1, inplace = True) # removed this unimportant feature\n",
        "\n",
        "# CLEANING OF DATA\n",
        "df_tele[\"TotalCharges\"] = df_tele[\"TotalCharges\"].replace(\" \", np.nan) # replacing empty strings np.nan, to that i can convert it to float\n",
        "df_tele.dropna(subset = [\"TotalCharges\"], inplace=True) # cleaded out the array by dropping does empty values within the Totalcharges feature\n",
        "df_tele[\"TotalCharges\"] = pd.to_numeric(df_tele[\"TotalCharges\"]) # coverted to float because of the kind of content within totalcharges\n",
        "\n",
        "# ENCODING OF ALL CATEGORICAL FEATURES\n",
        "encoded = pd.get_dummies(df_tele, drop_first= True ) # this is one-hot key encoding (we want to transform all our categorical columns into )\n",
        "encoded.head()\n",
        "\n",
        "# CHANGING THE DATATYPE OF BINARY CATEGORICAL FEATURES\n",
        "bool_cols = encoded.select_dtypes(include = \"bool\").columns #select all columns with bool datatype\n",
        "encoded[bool_cols] = encoded[bool_cols].astype(int) #change all of them to int (0 and 1)\n",
        "\n",
        "encoded[\"tenure\"] = encoded[\"tenure\"].astype(float)\n",
        "all_numerical_features = encoded.select_dtypes(include = np.number).columns.tolist()\n",
        "\n",
        "needed_numerical_feature = [feature for feature in all_numerical_features if encoded[feature].nunique()>2] # getting the numerical feature titles ready\n",
        "\n",
        "Labels = encoded[\"Churn_Yes\"]\n",
        "Features = encoded.drop(\"Churn_Yes\", axis = 1)\n",
        "\n",
        "# OBTANING OUR TRANING AND TEST DATA\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(Features,Labels, test_size = 0.2, random_state = 42 )\n",
        "# SCALLING AND PREPARING OUR TRAINING AND TESTING DATA\n",
        "Scaler = StandardScaler()\n",
        "# let us fit and tranform  our training numerical-features\n",
        "X_train[needed_numerical_feature] = Scaler.fit_transform(X_train[needed_numerical_feature])\n",
        "\n",
        "# let us apply the scaling parameters gotten from above and fit it to our testing numerical-features\n",
        "X_test[needed_numerical_feature] = Scaler.transform(X_test[needed_numerical_feature])\n",
        "\n",
        "log_regression = LogisticRegression() # is max_iter really needed?\n",
        "log_regression.fit(X_train, Y_train) # FIT THIS WHERE OUR WEIGHT AND BIAS ARE DERVIED FROM\n",
        "\n",
        "print(f\"Training Accuracy: {log_regression.score(X_train, Y_train)}\")\n",
        "print(f\"Testing Accuracy: {log_regression.score(X_test, Y_test)}\")\n",
        "\n",
        "\n",
        "# LOGISTC REGRESSION FROM SCRATCH\n",
        "\n",
        "class Logistic_regression:\n",
        "  def __init__(self, learning_rate = 0.01, n_iters = 1000):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.n_iters = n_iters\n",
        "\n",
        "  def sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "# This is responsible deriving weight and bias of a several feature(s)\n",
        "  def fit (self,X,Y):\n",
        "    self.num_samples, self.num_features = X.shape #lets count the number of features and dataitems(rows)\n",
        "    self.weights = np.zeros(self.num_features)\n",
        "    self.bias = 0\n",
        "\n",
        "    for _ in range(self.n_iters):\n",
        "      Y_num_prediction = np.dot(X, self.weights) + self.bias\n",
        "      Y_predicted = self.sigmoid(Y_num_prediction)\n",
        "\n",
        "      # implementing gradient descent\n",
        "\n",
        "      # we are implementing the average our two derivatives of the loss function\n",
        "      dw = np.dot((Y_predicted - Y),X.T)/self.num_samples\n",
        "      db = np.mean((Y_predicted - Y))\n",
        "\n",
        "      # adjust weights after each iteration with learning rate\n",
        "      self.weights = self.weights - self.learning_rate * dw\n",
        "      self.bias = self.bias - self.learning_rate * db\n",
        "\n",
        "      # stoping condition with eculidean normalisation\n",
        "      if np.linalg.norm(db) < 0.001 and np.linalg.norm(dw) < 0.001:\n",
        "        break\n",
        "\n",
        "  def predict(self, X, Y):\n",
        "    Y_num_prediction = np.dot(X, self.weights) + self.bias\n",
        "    Y_predicted = self.sigmoid(Y_num_prediction)\n",
        "    return Y_predicted\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSZYHGtHS1xS",
        "outputId": "dc7f4708-087f-441d-b793-00288a1901e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training Accuracy: 0.8090666666666667\n",
            "Testing Accuracy: 0.7867803837953091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vo0RQxz9f2bm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}